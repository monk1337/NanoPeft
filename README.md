# NanoPeft
The simplest repository & Neat implementation of different Lora methods for training/fine-tuning Transformer-based models (i.e., BERT, GPTs).
Keeping the code so simple, it is very easy to hack to your needs, add new Lora methods from papers in layers/ and finetune easily as per needs.
This is mostly for experiments/research purposes, not for scalable solutions. 
