<div align="center">
<h1>NanoPeft</h1>
</div>

<p align="center">
  <p align="center">The simplest repository & Neat implementation of different Lora methods for training/fine-tuning Transformer-based models (i.e., BERT, GPTs).
</p>


<h4 align="center">
  <a href="https://github.com/monk1337/NanoPeft/blob/main/LICENSE">
    <img src="https://img.shields.io/badge/License-Apache_2.0-blue.svg" alt="Apache 2.0 license." />
  </a>
  <a href="#">
    <img src="https://badge.fury.io/py/Promptify.svg" alt="PyPI version" />
  </a>
  <a href="http://makeapullrequest.com">
    <img src="https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square" alt="http://makeapullrequest.com" />
  </a>
  <a href="#">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="colab" />
  </a>
</h4>

<div align="center">
<img width="500px" src="https://raw.githubusercontent.com/monk1337/MultiMedQA/main/assets/nanopeft_final.png">
</div>

# Why NanoPeft?
- Hacking the Hugging Face PEFT (Parameter-Efficient Fine-Tuning) or LitGit packages seems like a lot of work to integrate a new LoRA method quickly and benchmark it.
- By keeping the code so simple, it is very easy to hack to your needs, add new LoRA methods from papers in the layers/ directory, and fine-tune easily as per your needs.
- This is mostly for experimental/research purposes, not for scalable solutions.



## Installation

### With pip


You should install NanoPeft using Pip command

```bash
pip3 install git+https://github.com/monk1337/NanoPeft.git
```
